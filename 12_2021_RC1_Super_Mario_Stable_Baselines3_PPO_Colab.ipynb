{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hululuzhu/mario-rl/blob/main/12_2021_RC1_Super_Mario_Stable_Baselines3_PPO_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCC5_P8kyuVP"
      },
      "source": [
        "# Welcome to try out this Super Mario RL colab\n",
        "- We used Stable Baselines3 PPO\n",
        "- We downsampled the images to speed up training\n",
        "- We choose best-1-of-20-episodes in evaluation (save_gif) to \"cheat\", a common\n",
        "practice is to \"average\" instead of \"best\", we tried to save time to see effect \n",
        "sooner\n",
        "- If you like to periodically save your model/gif, please mount to your Drive and update the path in \"core code\" section\n",
        "- For more info, check out https://github.com/hululuzhu/mario-rl\n",
        "- Slides: [1.5h 2022 version for students](https://docs.google.com/presentation/d/e/2PACX-1vQi060aTNB7PblBe8lwlrCoonhhhJuVI4uRQLu2CeTthz-WRJ3RkCSmIrTFPPh06SBquDIlgUfqi8h-/pub?start=false&loop=false&delayms=3000) or [1h 2021 version for my colleagues](https://docs.google.com/presentation/d/e/2PACX-1vRH3idUYN3IYpI7LmXL0_Y_VVkAtKUlOE2dUjVWcZokr2h7gKiBnKK1zDdKo5e5SqqBZtB198JW13Dq/pub?start=false&loop=false&delayms=3000)\n",
        "- todo: the reward function in evaluation seems broken, to be fixed as of 12/2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct1Bf95EDI9j"
      },
      "outputs": [],
      "source": [
        "# Check out https://pypi.org/project/gym-super-mario-bros/ for more info\n",
        "# 8 worlds and 4 stages = 32 levels (I only tried 1-1 so far)\n",
        "WORLD = 1  # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "STAGE = 1  # @param {type:\"slider\", min:1, max:4, step:1}\n",
        "LEVEL = f\"{WORLD}-{STAGE}\"\n",
        "# 0 high, 1 low, 2 down, 3 lowest\n",
        "QUALITY = 0 # @param {type:\"slider\", min:0, max:4, step:1}\n",
        "DEFAULT_GAME = f\"SuperMarioBros-{LEVEL}-v{QUALITY}\"\n",
        "# Simplest list of actions, see full list https://github.com/Kautenja/gym-super-mario-bros/blob/master/gym_super_mario_bros/actions.py\n",
        "MY_ACTIONS = [[\"right\"], [\"right\", \"A\"]]\n",
        "\n",
        "\n",
        "# Important to set the 2 params below to affect time, each 50k steps takes ~11min in Nvidia P100 GPU at Colab\n",
        "# Literature suggests  batches*each_batch_steps to exceed 10M to get reliably good Mario AI\n",
        "# change to higher number for real run, 2 for test\n",
        "batches = 2 # @param {type:\"integer\"}\n",
        "# Change to 500k or more to expect see real impact, 50 for test the flow\n",
        "each_batch_steps = 50 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLXHcegdzFEf"
      },
      "source": [
        "# Imports and support methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4iVkaRuH436Z"
      },
      "outputs": [],
      "source": [
        "# @title Install necessary packages, ~3 mins\n",
        "!pip install -q stable-baselines3[extra] > /dev/null 2>&1\n",
        "!pip install -q gym-super-mario-bros > /dev/null 2>&1\n",
        "# Virtual display needed for render function\n",
        "!pip install -q gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -q -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# To display gif natively\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Uy0FS44JT7CI"
      },
      "outputs": [],
      "source": [
        "# @title Creates virtual display\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 256x140x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "p09J0pKwDCgn"
      },
      "outputs": [],
      "source": [
        "# @title Imports for this colab, if seeing error, restart instance\n",
        "# ----Display and system needed----\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_docs.vis.embed as embed\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# ----Game Env and Transform needed----\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "import gym_super_mario_bros\n",
        "from gym.wrappers import FrameStack\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym.wrappers import FrameStack\n",
        "from torchvision import transforms\n",
        "\n",
        "# ----PPO RL Algorithm----\n",
        "from stable_baselines3 import PPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtSjEBm52Ad7"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5SZs_hlx-mAO"
      },
      "outputs": [],
      "source": [
        "# @title Support method to create env and downsample\n",
        "# Reference: https://github.com/pytorch/tutorials/blob/master/intermediate_source/mario_rl_tutorial.py\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = transforms.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        my_transforms = transforms.Compose(\n",
        "            [transforms.Resize(self.shape), transforms.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = my_transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Apply Wrappers to environment, notice we limit the action spaces to\n",
        "# MY_ACTIONS which only has 2 actions\n",
        "def build_env():\n",
        "  env = gym_super_mario_bros.make(DEFAULT_GAME)\n",
        "  env = SkipFrame(env, skip=4)\n",
        "  env = GrayScaleObservation(env)\n",
        "  env = ResizeObservation(env, shape=84)\n",
        "  env = FrameStack(env, num_stack=4)\n",
        "  env = JoypadSpace(env, MY_ACTIONS)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xy1ukLWJB4G0"
      },
      "outputs": [],
      "source": [
        "# @title Output one frame that will feed to model\n",
        "def see_world(test_env):\n",
        "  obs = test_env.reset()\n",
        "  for i in range(10):\n",
        "      action = test_env.action_space.sample()\n",
        "      obs, reward, done, info = test_env.step(action)\n",
        "\n",
        "  from matplotlib import pyplot as plt\n",
        "  if obs.shape[0] == 4:\n",
        "    plt.imshow(obs[-1], cmap='gray')\n",
        "  else:\n",
        "    plt.imshow(obs)\n",
        "  plt.show()\n",
        "  del test_env\n",
        "print(\"Human sees\")\n",
        "see_world(gym_super_mario_bros.make(DEFAULT_GAME))\n",
        "print(\"Model sees\")\n",
        "see_world(build_env())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7Zo0X4R5D4Oq"
      },
      "outputs": [],
      "source": [
        "# @title Support method to save GIF to evaluate. Please note we \"cheated\" by using best-1-of-20, ideally use average\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def save_gif(model, image_file, max_steps=2000):\n",
        "  best_img = []\n",
        "  all_rewards = []\n",
        "  best_reward = 0\n",
        "  for i in range(20): # choose 1 best out of 10\n",
        "    env = build_env()\n",
        "    screen = env.render(mode='rgb_array')\n",
        "    im = Image.fromarray(screen)\n",
        "    images = [im]\n",
        "    obs = env.reset()\n",
        "    cur_best_reward = 0\n",
        "    for i in range(1, max_steps + 1):\n",
        "      # Reformat lazyframe to numpy for predict method\n",
        "      b = torch.Tensor(4, 84, 84)\n",
        "      torch.stack(obs._frames, out=b)\n",
        "      action, _ = model.predict(b.numpy())\n",
        "      # print(\"action\", action)\n",
        "      # As of 09/2022, step func seems complain action as numpy scalar, so convert to int\n",
        "      obs, reward, done, _ = env.step(action.tolist())\n",
        "      cur_best_reward += reward\n",
        "      # Render screen every 8/4 = 2 steps\n",
        "      if i % 2 == 0:\n",
        "        screen = env.render(mode='rgb_array')\n",
        "        images.append(Image.fromarray(screen))\n",
        "      if done:\n",
        "        break\n",
        "    all_rewards.append(cur_best_reward)\n",
        "    if cur_best_reward > best_reward or (\n",
        "        cur_best_reward == best_reward and len(images) > len(best_img)\n",
        "    ):\n",
        "      best_reward = cur_best_reward\n",
        "      best_img = images\n",
        "  best_img[0].save(\n",
        "      image_file, save_all=True, append_images=best_img[1:], loop=0, duration=1)\n",
        "  print(\"mean reward of 20 episodes\", sum(all_rewards) / len(all_rewards), \"\\tlength\", len(best_img))\n",
        "  print(\"saved to\", image_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp63PQpg6KeR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Core training code!\n",
        "prefix = \"ppo_cnn_\"\n",
        "!mkdir -p \"/content/mario_rl/models\"\n",
        "!mkdir -p \"/content/mario_rl/videos\"\n",
        "\n",
        "model = PPO('CnnPolicy', build_env(), verbose=0)\n",
        "# Checkpoint if applicable, e.g. the 1.5M checkpoint in github dir\n",
        "# model.load(\"checkpoint_zip_path\") # if this line enabled, you may skip training below\n",
        "base_steps = 0 # 450_000 to map the zip file above\n",
        "total_steps = base_steps\n",
        "for i in range(1, 1 + batches):\n",
        "  obs = model.env.reset()\n",
        "  model.learn(total_timesteps=each_batch_steps)\n",
        "  total_steps += each_batch_steps\n",
        "  if each_batch_steps > 50000:  # only save model if the batch step > 50k\n",
        "    model.save(f\"/content/mario_rl/models/model_{total_steps}\")\n",
        "  save_gif(model, f\"/content/mario_rl/videos/model_{total_steps}.gif\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E3wkx4EP24oA"
      },
      "outputs": [],
      "source": [
        "# @title Check out model/gif outputs\n",
        "!ls /content/mario_rl -R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DxEM09WcNP5E"
      },
      "outputs": [],
      "source": [
        "# @title optionally to view the gif here\n",
        "embed.embed_file(f\"/content/mario_rl/videos/model_{total_steps}.gif\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "12/2021 RC1 Super Mario Stable Baselines3 PPO Colab",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}